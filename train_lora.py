#!/usr/bin/env python3
"""
ä½¿ç”¨LoRAå¾®è°ƒQwen2.5-0.5Bç”Ÿæˆæ»‘å—éªŒè¯ç è½¨è¿¹
"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training
)
from pathlib import Path

# è®¾ç½®
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"  # é˜¿é‡Œé€šä¹‰åƒé—®2.5 (0.5B) - 2024å¹´9æœˆå‘å¸ƒ
OUTPUT_DIR = Path(__file__).parent / "models"
DATA_FILE = Path(__file__).parent / "training_data.jsonl"

def tokenize_function(examples, tokenizer):
    """TokenåŒ–æ–‡æœ¬"""
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length"
    )

def main():
    print("ğŸš€ Starting LoRA fine-tuning for slider trajectory generation")
    print("=" * 70)

    # 1. åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ Loading dataset from {DATA_FILE}")
    dataset = load_dataset("json", data_files=str(DATA_FILE), split="train")
    print(f"âœ… Loaded {len(dataset)} samples")

    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    dataset = dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = dataset["train"]
    eval_dataset = dataset["test"]
    print(f"ğŸ“Š Train: {len(train_dataset)}, Eval: {len(eval_dataset)}")

    # 2. åŠ è½½Tokenizerå’Œæ¨¡å‹
    print(f"\nğŸ¤– Loading model: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # è®¾ç½®pad_tokenï¼ˆQwen2.5å·²æœ‰pad_tokenï¼Œä½†ä»ç¡®ä¿è®¾ç½®ï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map="auto",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        trust_remote_code=True  # Qwenéœ€è¦ä¿¡ä»»è¿œç¨‹ä»£ç 
    )
    print(f"âœ… Model loaded on device: {model.device}")

    # 3. TokenåŒ–æ•°æ®é›†
    print("\nğŸ”¤ Tokenizing dataset...")
    tokenized_train = train_dataset.map(
        lambda x: tokenize_function(x, tokenizer),
        batched=True,
        remove_columns=["text"]
    )
    tokenized_eval = eval_dataset.map(
        lambda x: tokenize_function(x, tokenizer),
        batched=True,
        remove_columns=["text"]
    )
    print("âœ… Tokenization complete")

    # 4. é…ç½®LoRA
    print("\nâš™ï¸  Configuring LoRA...")
    lora_config = LoraConfig(
        r=16,                   # LoRA rank (Qwenç”¨16æ•ˆæœæ›´å¥½)
        lora_alpha=32,          # LoRA alpha
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # Qwen2.5çš„attentionå’ŒFFNå±‚
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # 5. è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=str(OUTPUT_DIR),
        num_train_epochs=10,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        fp16=False,  # MacBookä¸æ”¯æŒCUDAï¼Œä½¿ç”¨float32
        logging_steps=50,
        save_steps=200,
        eval_steps=200,
        eval_strategy="steps",  # æ–°ç‰ˆæœ¬ä½¿ç”¨eval_strategyè€Œéevaluation_strategy
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        warmup_steps=100,
        report_to="none",
        remove_unused_columns=False
    )

    # 6. æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # Causal LMä¸ä½¿ç”¨MLM
    )

    # 7. åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_eval,
        data_collator=data_collator
    )

    # 8. å¼€å§‹è®­ç»ƒ
    print("\nğŸ¯ Starting training...")
    print("=" * 70)
    trainer.train()

    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nğŸ’¾ Saving final model...")
    model.save_pretrained(OUTPUT_DIR / "final_model")
    tokenizer.save_pretrained(OUTPUT_DIR / "final_model")
    print(f"âœ… Model saved to {OUTPUT_DIR / 'final_model'}")

    print("\nğŸ‰ Training complete!")
    print("=" * 70)

if __name__ == "__main__":
    main()
