#!/usr/bin/env python3
"""
ä½¿ç”¨å¾®è°ƒåçš„Qwen2.5-0.5Bæ¨¡å‹ç”Ÿæˆæ»‘å—è½¨è¿¹
"""
import datetime
import json

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from pathlib import Path
import re

class LLMTrajectoryGenerator:
    """åŸºäºLLMçš„è½¨è¿¹ç”Ÿæˆå™¨"""

    def __init__(self, model_path=None, device=None):
        """
        Args:
            model_path: å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ï¼ˆauto/cpu/cuda/mpsï¼‰
        """
        if model_path is None:
            model_path = Path(__file__).parent / "models/final_model"

        self.model_path = Path(model_path)

        if device is None:
            if torch.cuda.is_available():
                device = "cuda"
            elif torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"

        self.device = device
        print(f"ğŸ¤– Loading LLM model from {self.model_path}")
        print(f"ğŸ“ Device: {self.device}")

        # åŠ è½½tokenizerå’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            device_map=device,
            dtype=torch.float16 if device in ["cuda", "mps"] else torch.float32,
            trust_remote_code=True  # Qwenéœ€è¦ä¿¡ä»»è¿œç¨‹ä»£ç 
        )
        self.model.eval()
        print(f"âœ… Model loaded successfully")

    def generate(self, target_distance, canvas_length=280, temperature=0.8, top_p=0.95):
        """
        ç”Ÿæˆè½¨è¿¹

        Args:
            target_distance: ç›®æ ‡è·ç¦»
            canvas_length: ç”»å¸ƒé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦ï¼ˆè¶Šé«˜è¶Šå¤šæ ·ï¼‰
            top_p: nucleus samplingå‚æ•°

        Returns:
            tracks: List[Dict] - è½¨è¿¹ç‚¹ [{'a': x, 'b': y, 'c': dt}, ...]
        """
        # æ„å»ºè¾“å…¥
        input_text = f"<|input|>distance:{int(target_distance)},canvas:{canvas_length}<|output|>"

        # TokenåŒ–
        inputs = self.tokenizer(input_text, return_tensors="pt").to(self.device)

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.encode("<|end|>")[0] if "<|end|>" in self.tokenizer.get_vocab() else self.tokenizer.eos_token_id
            )

        # è§£ç 
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=False)

        # è§£æè½¨è¿¹
        tracks = self._parse_trajectory(generated_text, target_distance)

        return tracks

    def _parse_trajectory(self, text, target_distance):
        """
        ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­è§£æè½¨è¿¹

        æ ¼å¼: <|output|>0,0,0;5,-2,30;12,-3,28;...<|end|>
        """
        # æå–outputéƒ¨åˆ†
        match = re.search(r'<\|output\|>(.*?)(?:<\|end\|>|$)', text)
        if not match:
            print(f"âš ï¸  Failed to parse output, using fallback")
            return [{'a': 0, 'b': 0, 'c': 0}]

        output_str = match.group(1).strip()

        # è§£æè½¨è¿¹ç‚¹
        tracks = []
        points = output_str.split(';')

        for point_str in points:
            point_str = point_str.strip()
            if not point_str:
                continue

            try:
                parts = point_str.split(',')
                if len(parts) == 3:
                    x = int(float(parts[0]))
                    y = int(float(parts[1]))
                    dt = int(float(parts[2]))

                    # é™åˆ¶èŒƒå›´
                    x = max(0, min(x, int(target_distance * 1.1)))
                    y = max(-30, min(5, y))
                    dt = max(1, min(400, dt))

                    tracks.append({'a': x, 'b': y, 'c': dt})
            except (ValueError, IndexError) as e:
                # è·³è¿‡æ— æ•ˆç‚¹
                continue

        # ç¡®ä¿è‡³å°‘æœ‰èµ·ç‚¹
        if not tracks or tracks[0]['a'] != 0:
            tracks.insert(0, {'a': 0, 'b': 0, 'c': 0})

        # ç¡®ä¿ç»ˆç‚¹æ­£ç¡®
        if len(tracks) > 1:
            tracks[-1]['a'] = target_distance

        return tracks

def test_generator():
    """æµ‹è¯•ç”Ÿæˆå™¨"""
    print("ğŸ§ª Testing LLM Trajectory Generator")
    print("=" * 70)

    generator = LLMTrajectoryGenerator()

    # æµ‹è¯•ç”Ÿæˆ
    for i in range(3):
        print(f"\næµ‹è¯• {i+1}:")
        t = datetime.datetime.now()
        tracks = generator.generate(target_distance=60*(i+1), temperature=0.8)
        print(f"  ç”Ÿæˆç‚¹æ•°: {len(tracks)}")
        print(f"  æ€»æ—¶é—´: {sum(t['c'] for t in tracks)}ms")
        print(f"  ç‚¹: {json.dumps(tracks)}")
        tt = datetime.datetime.now() - t
        print(f"  <time>: {tt}")

if __name__ == "__main__":
    test_generator()
